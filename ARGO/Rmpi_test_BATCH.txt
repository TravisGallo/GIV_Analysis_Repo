#!/bin/sh

## Name of the job in the squeue output
#SBATCH --job-name RmpiHello

## Deal with output and errors.  Separate into 2 files (not the default).
## NOTE: %u=userID, %x=jobName, %N=nodeID, %j=jobID, %A=arrayMain, %a=arraySub
#SBATCH -o /scratch/%u/outputs/%x-%N-%j.out    # Output file
#SBATCH -e /scratch/%u/errors/%x-%N-%j.err    # Error file
#SBATCH --mail-type=BEGIN,END,FAIL     # NONE,BEGIN,END,FAIL,REQUEUE,ALL,...
#SBATCH --mail-user=hgallo@gmu.edu   # Put your GMU email address here

## Specifying an upper limit on needed resources will improve your scheduling
## priority, but if you exceed these values, your job will be terminated.
## Check your "Job Ended" emails for actual resource usage info.
#SBATCH --mem=4098MB    # Total memory needed for your job (suffixes: K,M,G,T)
#SBATCH --time=0-00:05  # Total time needed for your job: Days-Hours:Minutes

## How many nodes and how many cores per node
## This configuration will give is access to 20 cpus spread over 5 machines
#SBATCH --nodes=5
#SBATCH --ntasks-per-node=4

## Purge modules first
module purge

## Load the R module which also loads the OpenBLAS module
module load R/3.4.1
## To use Rmpi, you need to load the openmpi module
module load gcc/5.2.0
module load openmpi/3.1.0

## R wants to write files to our current directory, so make sure it's writable
##ORIG_DIR=$PWD
##cd /scratch/$USER

echo "Calling mpirun now!!!"


## note you do not spawn the parallel processes directly through
## mpirun but from inside your R script, hence parameter -np is set to 1
mpirun -np 1 Rscript testMPI.R
